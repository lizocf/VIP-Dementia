{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8baf7b35-77f3-4523-8520-c316f290e158",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/VIP-Dementia\n"
     ]
    }
   ],
   "source": [
    "cd /home/ec2-user/SageMaker/VIP-Dementia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "212e08ab-2aea-43d8-a7ae-9347aa57e4c9",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (1.22.4)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (1.4.1.post1)\n",
      "Requirement already satisfied: json5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (0.9.24)\n",
      "Collecting librosa (from -r requirements.txt (line 5))\n",
      "  Using cached librosa-0.10.1-py3-none-any.whl.metadata (8.3 kB)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (1.3.2)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (2.2.1)\n",
      "Collecting opensmile (from -r requirements.txt (line 8))\n",
      "  Using cached opensmile-2.5.0-py3-none-manylinux_2_17_x86_64.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from -r requirements.txt (line 9)) (4.66.2)\n",
      "Collecting xgboost (from -r requirements.txt (line 10))\n",
      "  Using cached xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting lightgbm (from -r requirements.txt (line 11))\n",
      "  Using cached lightgbm-4.3.0.tar.gz (1.7 MB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: scipy>=1.6.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 2)) (1.12.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from scikit-learn->-r requirements.txt (line 2)) (3.4.0)\n",
      "Collecting audioread>=2.1.9 (from librosa->-r requirements.txt (line 5))\n",
      "  Using cached audioread-3.0.1-py3-none-any.whl.metadata (8.4 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 5)) (5.1.1)\n",
      "Requirement already satisfied: numba>=0.51.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 5)) (0.59.1)\n",
      "Collecting soundfile>=0.12.1 (from librosa->-r requirements.txt (line 5))\n",
      "  Using cached soundfile-0.12.1-py2.py3-none-manylinux_2_17_x86_64.whl.metadata (14 kB)\n",
      "Collecting pooch>=1.0 (from librosa->-r requirements.txt (line 5))\n",
      "  Using cached pooch-1.8.1-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa->-r requirements.txt (line 5))\n",
      "  Using cached soxr-0.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 5)) (4.10.0)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 5)) (0.3)\n",
      "Requirement already satisfied: msgpack>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from librosa->-r requirements.txt (line 5)) (1.0.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 7)) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 7)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pandas->-r requirements.txt (line 7)) (2024.1)\n",
      "Collecting audobject>=0.6.1 (from opensmile->-r requirements.txt (line 8))\n",
      "  Using cached audobject-0.7.11-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting audinterface>=0.7.0 (from opensmile->-r requirements.txt (line 8))\n",
      "  Using cached audinterface-1.2.1-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting audeer>=1.18.0 (from audinterface>=0.7.0->opensmile->-r requirements.txt (line 8))\n",
      "  Using cached audeer-2.0.0-py3-none-any.whl.metadata (4.1 kB)\n",
      "Collecting audformat<2.0.0,>=1.0.1 (from audinterface>=0.7.0->opensmile->-r requirements.txt (line 8))\n",
      "  Using cached audformat-1.1.2-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting audiofile>=1.3.0 (from audinterface>=0.7.0->opensmile->-r requirements.txt (line 8))\n",
      "  Using cached audiofile-1.4.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting audmath>=1.3.0 (from audinterface>=0.7.0->opensmile->-r requirements.txt (line 8))\n",
      "  Using cached audmath-1.4.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting audresample<2.0.0,>=1.1.0 (from audinterface>=0.7.0->opensmile->-r requirements.txt (line 8))\n",
      "  Using cached audresample-1.3.3-py3-none-manylinux_2_17_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.8.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from audobject>=0.6.1->opensmile->-r requirements.txt (line 8)) (6.11.0)\n",
      "Collecting oyaml (from audobject>=0.6.1->opensmile->-r requirements.txt (line 8))\n",
      "  Using cached oyaml-1.0-py2.py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from audobject>=0.6.1->opensmile->-r requirements.txt (line 8)) (21.3)\n",
      "Requirement already satisfied: llvmlite<0.43,>=0.42.0dev0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from numba>=0.51.0->librosa->-r requirements.txt (line 5)) (0.42.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pooch>=1.0->librosa->-r requirements.txt (line 5)) (4.2.0)\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from pooch>=1.0->librosa->-r requirements.txt (line 5)) (2.31.0)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 7)) (1.16.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from soundfile>=0.12.1->librosa->-r requirements.txt (line 5)) (1.16.0)\n",
      "Collecting iso-639 (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile->-r requirements.txt (line 8))\n",
      "  Using cached iso-639-0.4.5.tar.gz (167 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting iso3166 (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile->-r requirements.txt (line 8))\n",
      "  Using cached iso3166-2.1.1-py3-none-any.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: pyyaml>=5.4.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from audformat<2.0.0,>=1.0.1->audinterface>=0.7.0->opensmile->-r requirements.txt (line 8)) (6.0.1)\n",
      "Requirement already satisfied: pycparser in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->-r requirements.txt (line 5)) (2.21)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from importlib-metadata>=4.8.0->audobject>=0.6.1->opensmile->-r requirements.txt (line 8)) (3.17.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from packaging->audobject>=0.6.1->opensmile->-r requirements.txt (line 8)) (3.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa->-r requirements.txt (line 5)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa->-r requirements.txt (line 5)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa->-r requirements.txt (line 5)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/python3/lib/python3.10/site-packages (from requests>=2.19.0->pooch>=1.0->librosa->-r requirements.txt (line 5)) (2024.2.2)\n",
      "Using cached librosa-0.10.1-py3-none-any.whl (253 kB)\n",
      "Using cached opensmile-2.5.0-py3-none-manylinux_2_17_x86_64.whl (996 kB)\n",
      "Using cached xgboost-2.0.3-py3-none-manylinux2014_x86_64.whl (297.1 MB)\n",
      "Using cached audinterface-1.2.1-py3-none-any.whl (66 kB)\n",
      "Using cached audioread-3.0.1-py3-none-any.whl (23 kB)\n",
      "Using cached audobject-0.7.11-py3-none-any.whl (43 kB)\n",
      "Using cached pooch-1.8.1-py3-none-any.whl (62 kB)\n",
      "Using cached soundfile-0.12.1-py2.py3-none-manylinux_2_17_x86_64.whl (1.3 MB)\n",
      "Using cached soxr-0.3.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "Using cached audeer-2.0.0-py3-none-any.whl (39 kB)\n",
      "Using cached audformat-1.1.2-py3-none-any.whl (140 kB)\n",
      "Using cached audiofile-1.4.0-py3-none-any.whl (1.1 MB)\n",
      "Using cached audmath-1.4.0-py3-none-any.whl (23 kB)\n",
      "Using cached audresample-1.3.3-py3-none-manylinux_2_17_x86_64.whl (138 kB)\n",
      "Using cached oyaml-1.0-py2.py3-none-any.whl (3.0 kB)\n",
      "Using cached iso3166-2.1.1-py3-none-any.whl (9.8 kB)\n",
      "Building wheels for collected packages: lightgbm, iso-639\n",
      "  Building wheel for lightgbm (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for lightgbm: filename=lightgbm-4.3.0-py3-none-linux_x86_64.whl size=2461508 sha256=79cf7e37e526452f6cacfe6ab981f6e2fdebe6e4396581d0acf4c387f589748e\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/6b/92/ab/b7b5df76502b64443c1a830e5f7ec3cb66741313ddebb682aa\n",
      "  Building wheel for iso-639 (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for iso-639: filename=iso_639-0.4.5-py3-none-any.whl size=168840 sha256=b877c34ffc14091ce941679c02f92ee3d47b8539f714848a94303e43fcecf9de\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/d8/78/cc/5478ca3b1c3f602eae6f8cdbd78f909c0a0bfa0bbcb5c7771f\n",
      "Successfully built lightgbm iso-639\n",
      "Installing collected packages: iso-639, soxr, oyaml, iso3166, audresample, audmath, audioread, audeer, xgboost, soundfile, pooch, lightgbm, audobject, librosa, audiofile, audformat, audinterface, opensmile\n",
      "Successfully installed audeer-2.0.0 audformat-1.1.2 audinterface-1.2.1 audiofile-1.4.0 audioread-3.0.1 audmath-1.4.0 audobject-0.7.11 audresample-1.3.3 iso-639-0.4.5 iso3166-2.1.1 librosa-0.10.1 lightgbm-4.3.0 opensmile-2.5.0 oyaml-1.0 pooch-1.8.1 soundfile-0.12.1 soxr-0.3.7 xgboost-2.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f6f937c7-c5e9-45af-ba45-4392321a009f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker/VIP-Dementia/audio_processing\n"
     ]
    }
   ],
   "source": [
    "cd /home/ec2-user/SageMaker/VIP-Dementia/audio_processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6bcc1b15-7d98-4f1f-8265-af1c41553aad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved X_train.npy to /home/ec2-user/SageMaker/VIP-Dementia/data/processed\n",
      "Saved X_valid.npy to /home/ec2-user/SageMaker/VIP-Dementia/data/processed\n",
      "Saved X_test.npy to /home/ec2-user/SageMaker/VIP-Dementia/data/processed\n",
      "Saved y_train.npy to /home/ec2-user/SageMaker/VIP-Dementia/data/processed\n",
      "Saved y_valid.npy to /home/ec2-user/SageMaker/VIP-Dementia/data/processed\n",
      "Saved y_test.npy to /home/ec2-user/SageMaker/VIP-Dementia/data/processed\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Specify the directory where you want to save the files\n",
    "HOME_DIRECTORY = os.path.expanduser(\"~/SageMaker/VIP-Dementia\")\n",
    "save_dir = os.path.join(HOME_DIRECTORY, 'data/processed')\n",
    "\n",
    "# Function to save datasets\n",
    "def save_dataset(filename, dataset):\n",
    "    path = os.path.join(save_dir, filename)\n",
    "    np.save(path, dataset)\n",
    "    print(f\"Saved {filename} to {save_dir}\")\n",
    "\n",
    "# Saving each dataset\n",
    "save_dataset('X_train.npy', X_train)\n",
    "save_dataset('X_valid.npy', X_valid)\n",
    "save_dataset('X_test.npy', X_test)\n",
    "save_dataset('y_train.npy', y_train)\n",
    "save_dataset('y_valid.npy', y_valid)\n",
    "save_dataset('y_test.npy', y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3c5bf3a3-5e96-451e-bccb-e4fb5fe5c714",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded X_train: (183597, 88), y_train: (183597,)\n",
      "Loaded X_test: (45900, 88), y_test: (45900,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Specify the directory where you want to save the files\n",
    "HOME_DIRECTORY = os.path.expanduser(\"~/SageMaker/VIP-Dementia\")\n",
    "SPLIT_DATA_DIR = os.path.join(HOME_DIRECTORY, 'data/processed')\n",
    "\n",
    "# Function to load datasets\n",
    "def load_dataset(filename):\n",
    "    path = os.path.join(SPLIT_DATA_DIR, filename)\n",
    "    if os.path.exists(path):\n",
    "        return np.load(path, allow_pickle=True)\n",
    "    else:\n",
    "        print(f\"File {filename} not found in {SPLIT_DATA_DIR}. Please check the directory and try again.\")\n",
    "        return None\n",
    "\n",
    "# Loading each dataset\n",
    "X_train = load_dataset('X_train.npy')\n",
    "X_test = load_dataset('X_test.npy')\n",
    "y_train = load_dataset('y_train.npy')\n",
    "y_test = load_dataset('y_test.npy')\n",
    "\n",
    "# Verifying the shapes of the loaded datasets (if they were successfully loaded)\n",
    "if X_train is not None and y_train is not None:\n",
    "    print(f\"Loaded X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "if X_test is not None and y_test is not None:\n",
    "    print(f\"Loaded X_test: {X_test.shape}, y_test: {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d2e639b3-a865-4cf8-8f8c-866d5445a929",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_curve, roc_auc_score, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"XGBoost\": xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss'),\n",
    "    \"LightGBM\": lgb.LGBMClassifier()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d3585877-de45-40ae-9038-de4aa9b4f4c2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 109702, number of negative: 73895\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.057820 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 21030\n",
      "[LightGBM] [Info] Number of data points in the train set: 183597, number of used features: 88\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.597515 -> initscore=0.395122\n",
      "[LightGBM] [Info] Start training from score 0.395122\n",
      "All models trained and predictions stored.\n"
     ]
    }
   ],
   "source": [
    "model_outputs = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)  # Train model\n",
    "    probs = model.predict_proba(X_test)[:, 1]\n",
    "    preds = model.predict(X_test)\n",
    "    model_outputs[name] = (probs, preds)\n",
    "print(\"All models trained and predictions stored.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5dc4e665-7656-4a8c-81e9-ee1e3f489854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "def get_roc_curve(y_test, probs, model_name, color):\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, probs)\n",
    "    plt.plot(fpr, tpr, linestyle='-', color=color, label=f'{model_name}', linewidth=0.7)\n",
    "    plt.xlabel('False Positive Rate', fontsize=8)\n",
    "    plt.ylabel('True Positive Rate', fontsize=8)\n",
    "    plt.xticks(fontsize=8)\n",
    "    plt.yticks(fontsize=8)\n",
    "    plt.legend(loc='lower right', fontsize=8)\n",
    "\n",
    "def get_confusion_matrix(y_test, y_pred, class_names, model_name):\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "    cm_percent = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]  \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8, 6)) \n",
    "    sns.heatmap(cm_percent, annot=True, fmt=\".2%\", cmap='Blues', xticklabels=class_names, yticklabels=class_names, ax=ax)\n",
    "    plt.title(f'Confusion Matrix for {model_name}')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.show()\n",
    "\n",
    "def get_performance_metrics(y_test, model_outputs):\n",
    "    metrics = {\n",
    "        'Model': [],\n",
    "        'Accuracy (%)': [],\n",
    "        'Precision (%)': [],\n",
    "        'Recall (%)': [],\n",
    "        'F1-score (%)': []\n",
    "    }\n",
    "\n",
    "    for name, outputs in model_outputs.items():\n",
    "        y_pred = outputs[1]\n",
    "        metrics['Model'].append(name)\n",
    "        metrics['Accuracy (%)'].append(100 * accuracy_score(y_test, y_pred))\n",
    "        metrics['Precision (%)'].append(100 * precision_score(y_test, y_pred, average=\"macro\"))\n",
    "        metrics['Recall (%)'].append(100 * recall_score(y_test, y_pred, average=\"macro\"))\n",
    "        metrics['F1-score (%)'].append(100 * f1_score(y_test, y_pred, average=\"macro\"))\n",
    "\n",
    "    results_df = pd.DataFrame(metrics)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f4ca8fc9-3982-48ce-a48c-6ea367ddd23a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Model  Accuracy (%)  Precision (%)  Recall (%)  F1-score (%)\n",
      "0  Random Forest     87.612200      87.562190   86.520756     86.953444\n",
      "1        XGBoost     79.873638      79.296951   78.460295     78.795477\n",
      "2       LightGBM     76.435730      75.913387   74.321941     74.819715\n"
     ]
    }
   ],
   "source": [
    "results_df = get_performance_metrics(y_test, model_outputs)\n",
    "print(results_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43390a6c-46f5-40ac-91d8-1845a24d9424",
   "metadata": {},
   "source": [
    "## RandomForest Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db46ed61-0ad0-4f68-9054-01591cdc05ec",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default Random Forest Model's accuracy on training set is 100.00%\n",
      "Default Random Forest Model's accuracy on test set is 94.12%\n",
      "\n"
     ]
    },
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'oob_score' parameter of RandomForestClassifier must be an instance of 'bool' or an instance of 'numpy.bool_' or a callable. Got 'True' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 24\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m########## Tuned Random Forest #######\u001b[39;00m\n\u001b[1;32m     15\u001b[0m model \u001b[38;5;241m=\u001b[39m RandomForestClassifier(\n\u001b[1;32m     16\u001b[0m     n_estimators \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m500\u001b[39m, \n\u001b[1;32m     17\u001b[0m     criterion \u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m69\u001b[39m  \n\u001b[1;32m     22\u001b[0m ) \n\u001b[0;32m---> 24\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest Model\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms accuracy on training set is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mscore(X_train,\u001b[38;5;250m \u001b[39my_train)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest Model\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124ms accuracy on test set is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;241m100\u001b[39m\u001b[38;5;241m*\u001b[39mmodel\u001b[38;5;241m.\u001b[39mscore(X_test,\u001b[38;5;250m \u001b[39my_test)\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/base.py:1467\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1463\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[1;32m   1464\u001b[0m )\n\u001b[1;32m   1466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[0;32m-> 1467\u001b[0m     \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[1;32m   1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/base.py:666\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    659\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[1;32m    660\u001b[0m \n\u001b[1;32m    661\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    664\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[1;32m    665\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 666\u001b[0m     \u001b[43mvalidate_parameter_constraints\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    667\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parameter_constraints\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    668\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    669\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcaller_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    670\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/python3/lib/python3.10/site-packages/sklearn/utils/_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[0;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     93\u001b[0m     )\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[1;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     98\u001b[0m )\n",
      "\u001b[0;31mInvalidParameterError\u001b[0m: The 'oob_score' parameter of RandomForestClassifier must be an instance of 'bool' or an instance of 'numpy.bool_' or a callable. Got 'True' instead."
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "####### Default Random Forest ########\n",
    "model = RandomForestClassifier(\n",
    "    random_state=69\n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f'Default Random Forest Model\\'s accuracy on training set is {100*model.score(X_train, y_train):.2f}%')\n",
    "print(f'Default Random Forest Model\\'s accuracy on test set is {100*model.score(X_test, y_test):.2f}%\\n')\n",
    "\n",
    "\n",
    "########## Tuned Random Forest #######\n",
    "model = RandomForestClassifier(\n",
    "    n_estimators = 500, \n",
    "    criterion ='entropy',\n",
    "    warm_start = True,\n",
    "    max_features = 'sqrt',\n",
    "    oob_score = 'True', # more on this below\n",
    "    random_state=69  \n",
    ") \n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f'Random Forest Model\\'s accuracy on training set is {100*model.score(X_train, y_train):.2f}%')\n",
    "print(f'Random Forest Model\\'s accuracy on test set is {100*model.score(X_test, y_test):.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a85f371e-d148-48c5-8fc6-22be3c905ad2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Light GBM Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7b53e3f2-7f88-4335-a722-deaf24104a59",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Default LGBM's accuracy on training set is 100.00%\n",
      "Default LGBM's accuracy on test set is 82.35%\n",
      "\n",
      "Tuned LGBM's accuracy on training set is 100.00%\n",
      "Tuned LGBM's accuracy on test set is 88.24%\n"
     ]
    }
   ],
   "source": [
    "####### Default LGBM ########\n",
    "model = lgb.LGBMClassifier(\n",
    "    verbosity=-1,\n",
    "    random_state=69\n",
    ")\n",
    "\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(f'Default LGBM\\'s accuracy on training set is {100*model.score(X_train, y_train):.2f}%')\n",
    "print(f'Default LGBM\\'s accuracy on test set is {100*model.score(X_test, y_test):.2f}%\\n')\n",
    "\n",
    "\n",
    "########## Tuned LGBM #######\n",
    "\n",
    "params = {\n",
    "    'application': 'binary', # for binary classification\n",
    "#     'num_class' : 1, # used for multi-classes\n",
    "    'boosting': 'gbdt', # traditional gradient boosting decision tree\n",
    "    'num_iterations': 100, \n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 62,\n",
    "    'device': 'cpu', # you can use GPU to achieve faster learning\n",
    "    'max_depth': -1, # <0 means no limit\n",
    "    'max_bin': 510, # Small number of bins may reduce training accuracy but can deal with over-fitting\n",
    "    'lambda_l1': 5, # L1 regularization\n",
    "    'lambda_l2': 10, # L2 regularization\n",
    "    'metric' : 'binary_error',\n",
    "    'subsample_for_bin': 200, # number of samples for constructing bins\n",
    "    'subsample': 1, # subsample ratio of the training instance\n",
    "    'colsample_bytree': 0.8, # subsample ratio of columns when constructing the tree\n",
    "    'min_split_gain': 0.5, # minimum loss reduction required to make further partition on a leaf node of the tree\n",
    "    'min_child_weight': 1, # minimum sum of instance weight (hessian) needed in a leaf\n",
    "    'min_child_samples': 5# minimum number of data needed in a leaf\n",
    "}\n",
    "\n",
    "# Initiate classifier to use\n",
    "tuned_model = lgb.LGBMClassifier(boosting_type= 'gbdt', \n",
    "          objective = 'binary', \n",
    "          n_jobs = 5, \n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'], \n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'], \n",
    "          min_split_gain = params['min_split_gain'], \n",
    "          min_child_weight = params['min_child_weight'], \n",
    "          min_child_samples = params['min_child_samples'],\n",
    "          verbosity=-1)\n",
    "\n",
    "tuned_model.fit(X_train, y_train)\n",
    "\n",
    "print(f'Tuned LGBM\\'s accuracy on training set is {100*tuned_model.score(X_train, y_train):.2f}%')\n",
    "print(f'Tuned LGBM\\'s accuracy on test set is {100*tuned_model.score(X_test, y_test):.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9750e002-7aec-49ba-834d-3885e775c0fa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 4 folds for each of 3456 candidates, totalling 13824 fits\n",
      "{'boosting_type': 'dart', 'colsample_bytree': 0.65, 'learning_rate': 0.01, 'max_bin': 255, 'n_estimators': 16, 'num_leaves': 6, 'objective': 'binary', 'random_state': 500, 'reg_alpha': 1.2, 'reg_lambda': 1.4, 'subsample': 0.7}\n",
      "0.5757575757575757\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gridParams = {\n",
    "    'learning_rate': [0.005, 0.01],\n",
    "    'n_estimators': [8,16,24],\n",
    "    'num_leaves': [6,8,12,16], # large num_leaves helps improve accuracy but might lead to over-fitting\n",
    "    'boosting_type' : ['gbdt', 'dart'], # for better accuracy -> try dart\n",
    "    'objective' : ['binary'],\n",
    "    'max_bin':[255, 510], # large max_bin helps improve accuracy but might slow down training progress\n",
    "    'random_state' : [500],\n",
    "    'colsample_bytree' : [0.64, 0.65, 0.66],\n",
    "    'subsample' : [0.7,0.75],\n",
    "    'reg_alpha' : [1,1.2],\n",
    "    'reg_lambda' : [1,1.2,1.4],\n",
    "    }\n",
    "\n",
    "grid = GridSearchCV(model, gridParams, verbose=1, cv=4, n_jobs=-1)\n",
    "# Run the grid\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# Print the best parameters found\n",
    "print(grid.best_params_)\n",
    "print(grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0eea07ae-b17e-416b-a271-955c18c3d426",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'application': 'binary',\n",
       " 'boosting': 'gbdt',\n",
       " 'num_iterations': 100,\n",
       " 'learning_rate': 0.01,\n",
       " 'num_leaves': 6,\n",
       " 'device': 'cpu',\n",
       " 'max_depth': -1,\n",
       " 'max_bin': 255,\n",
       " 'lambda_l1': 5,\n",
       " 'lambda_l2': 10,\n",
       " 'metric': 'binary_error',\n",
       " 'subsample_for_bin': 200,\n",
       " 'subsample': 0.7,\n",
       " 'colsample_bytree': 0.65,\n",
       " 'min_split_gain': 0.5,\n",
       " 'min_child_weight': 1,\n",
       " 'min_child_samples': 5,\n",
       " 'reg_alpha': 1.2,\n",
       " 'reg_lambda': 1.4}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params['colsample_bytree'] = grid.best_params_['colsample_bytree']\n",
    "params['learning_rate'] = grid.best_params_['learning_rate'] \n",
    "params['max_bin'] = grid.best_params_['max_bin']\n",
    "params['num_leaves'] = grid.best_params_['num_leaves']\n",
    "params['reg_alpha'] = grid.best_params_['reg_alpha']\n",
    "params['reg_lambda'] = grid.best_params_['reg_lambda']\n",
    "params['subsample'] = grid.best_params_['subsample']\n",
    "\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "751e9e4b-9526-4770-85f4-68ed1f92ad6d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuned LGBM's accuracy on training set is 100.00%\n",
      "Tuned LGBM's accuracy on test set is 88.24%\n"
     ]
    }
   ],
   "source": [
    "# Initiate classifier to use\n",
    "grid_model = lgb.LGBMClassifier(boosting_type= 'dart', \n",
    "          objective = 'binary', \n",
    "          n_jobs = 5, \n",
    "          silent = True,\n",
    "          max_depth = params['max_depth'],\n",
    "          max_bin = params['max_bin'], \n",
    "          subsample_for_bin = params['subsample_for_bin'],\n",
    "          subsample = params['subsample'], \n",
    "          min_split_gain = params['min_split_gain'], \n",
    "          min_child_weight = params['min_child_weight'], \n",
    "          min_child_samples = params['min_child_samples'],\n",
    "          verbosity=-1)\n",
    "\n",
    "grid_model.fit(X_train, y_train)\n",
    "\n",
    "print(f'Tuned LGBM\\'s accuracy on training set is {100*grid_model.score(X_train, y_train):.2f}%')\n",
    "print(f'Tuned LGBM\\'s accuracy on test set is {100*grid_model.score(X_test, y_test):.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
